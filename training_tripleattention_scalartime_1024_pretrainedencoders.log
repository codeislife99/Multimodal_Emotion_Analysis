Parameters in the model = 88
Training -- Epoch [1], Sample [1], Average Loss: 0.8890
Training -- Epoch [1], Sample [2], Average Loss: 0.9367
Training -- Epoch [1], Sample [3], Average Loss: 0.6245
Training -- Epoch [1], Sample [4], Average Loss: 0.7184
Training -- Epoch [1], Sample [5], Average Loss: 0.7525
Training -- Epoch [1], Sample [6], Average Loss: 0.6271
Training -- Epoch [1], Sample [7], Average Loss: 0.5526
Training -- Epoch [1], Sample [8], Average Loss: 0.5381
Training -- Epoch [1], Sample [9], Average Loss: 0.4783
Training -- Epoch [1], Sample [10], Average Loss: 0.4411
Training -- Epoch [1], Sample [11], Average Loss: 0.8923
Training -- Epoch [1], Sample [12], Average Loss: 0.8180
Training -- Epoch [1], Sample [13], Average Loss: 0.8901
Training -- Epoch [1], Sample [14], Average Loss: 0.8345
Training -- Epoch [1], Sample [15], Average Loss: 0.8515
Training -- Epoch [1], Sample [16], Average Loss: 1.1360
Training -- Epoch [1], Sample [17], Average Loss: 1.0754
Training -- Epoch [1], Sample [18], Average Loss: 1.0953
Training -- Epoch [1], Sample [19], Average Loss: 1.1299
Training -- Epoch [1], Sample [20], Average Loss: 1.0786
Training -- Epoch [1], Sample [21], Average Loss: 1.0323
Training -- Epoch [1], Sample [22], Average Loss: 0.9854
Training -- Epoch [1], Sample [23], Average Loss: 1.0188
Training -- Epoch [1], Sample [24], Average Loss: 0.9763
Training -- Epoch [1], Sample [25], Average Loss: 0.9590
Training -- Epoch [1], Sample [26], Average Loss: 0.9598
Training -- Epoch [1], Sample [27], Average Loss: 0.9282
Training -- Epoch [1], Sample [28], Average Loss: 0.9030
Training -- Epoch [1], Sample [29], Average Loss: 0.8718
Training -- Epoch [1], Sample [30], Average Loss: 0.8572
Training -- Epoch [1], Sample [31], Average Loss: 0.8860
Training -- Epoch [1], Sample [32], Average Loss: 0.9821
Training -- Epoch [1], Sample [33], Average Loss: 1.0054
Training -- Epoch [1], Sample [34], Average Loss: 0.9789
Training -- Epoch [1], Sample [35], Average Loss: 0.9666
Training -- Epoch [1], Sample [36], Average Loss: 0.9398
Training -- Epoch [1], Sample [37], Average Loss: 0.9144
Training -- Epoch [1], Sample [38], Average Loss: 0.9363
Training -- Epoch [1], Sample [39], Average Loss: 0.9150
Training -- Epoch [1], Sample [40], Average Loss: 0.8921
Training -- Epoch [1], Sample [41], Average Loss: 0.8917
Training -- Epoch [1], Sample [42], Average Loss: 0.8705
Training -- Epoch [1], Sample [43], Average Loss: 0.8554
Training -- Epoch [1], Sample [44], Average Loss: 0.8484
Training -- Epoch [1], Sample [45], Average Loss: 0.8319
Training -- Epoch [1], Sample [46], Average Loss: 0.8518
Training -- Epoch [1], Sample [47], Average Loss: 0.8919
Training -- Epoch [1], Sample [48], Average Loss: 0.8823
Training -- Epoch [1], Sample [49], Average Loss: 0.8664
Training -- Epoch [1], Sample [50], Average Loss: 0.8532
Training -- Epoch [1], Sample [51], Average Loss: 0.8387
Training -- Epoch [1], Sample [52], Average Loss: 0.8434
Training -- Epoch [1], Sample [53], Average Loss: 0.8378
Training -- Epoch [1], Sample [54], Average Loss: 0.8223
Training -- Epoch [1], Sample [55], Average Loss: 0.8133
Training -- Epoch [1], Sample [56], Average Loss: 0.8008
Training -- Epoch [1], Sample [57], Average Loss: 0.7942
Training -- Epoch [1], Sample [58], Average Loss: 0.7879
Training -- Epoch [1], Sample [59], Average Loss: 0.8096
Training -- Epoch [1], Sample [60], Average Loss: 0.7961
Training -- Epoch [1], Sample [61], Average Loss: 0.8116
Training -- Epoch [1], Sample [62], Average Loss: 0.7985
Training -- Epoch [1], Sample [63], Average Loss: 0.8134
Training -- Epoch [1], Sample [64], Average Loss: 0.8024
Training -- Epoch [1], Sample [65], Average Loss: 0.8051
Training -- Epoch [1], Sample [66], Average Loss: 0.7929
Training -- Epoch [1], Sample [67], Average Loss: 0.7941
Training -- Epoch [1], Sample [68], Average Loss: 0.7968
Training -- Epoch [1], Sample [69], Average Loss: 0.7915
Training -- Epoch [1], Sample [70], Average Loss: 0.8566
Training -- Epoch [1], Sample [71], Average Loss: 0.8520
Training -- Epoch [1], Sample [72], Average Loss: 0.8402
Training -- Epoch [1], Sample [73], Average Loss: 0.8287
Training -- Epoch [1], Sample [74], Average Loss: 0.8406
Training -- Epoch [1], Sample [75], Average Loss: 0.8813
Training -- Epoch [1], Sample [76], Average Loss: 0.8697
Training -- Epoch [1], Sample [77], Average Loss: 0.8584
Training -- Epoch [1], Sample [78], Average Loss: 0.8517
Training -- Epoch [1], Sample [79], Average Loss: 0.8435
Training -- Epoch [1], Sample [80], Average Loss: 0.8441
Training -- Epoch [1], Sample [81], Average Loss: 0.8664
Training -- Epoch [1], Sample [82], Average Loss: 0.8613
Training -- Epoch [1], Sample [83], Average Loss: 0.8509
Training -- Epoch [1], Sample [84], Average Loss: 0.8712
Training -- Epoch [1], Sample [85], Average Loss: 0.8714
Training -- Epoch [1], Sample [86], Average Loss: 0.8718
Training -- Epoch [1], Sample [87], Average Loss: 0.8848
Training -- Epoch [1], Sample [88], Average Loss: 0.8748
Training -- Epoch [1], Sample [89], Average Loss: 0.8764
Training -- Epoch [1], Sample [90], Average Loss: 0.8840
Training -- Epoch [1], Sample [91], Average Loss: 0.8840
Training -- Epoch [1], Sample [92], Average Loss: 0.8751
Training -- Epoch [1], Sample [93], Average Loss: 0.8797
Training -- Epoch [1], Sample [94], Average Loss: 0.9067
Training -- Epoch [1], Sample [95], Average Loss: 0.9438
Training -- Epoch [1], Sample [96], Average Loss: 0.9565
Training -- Epoch [1], Sample [97], Average Loss: 0.9511
Training -- Epoch [1], Sample [98], Average Loss: 0.9502
Training -- Epoch [1], Sample [99], Average Loss: 0.9493
Training -- Epoch [1], Sample [100], Average Loss: 0.9901
Training -- Epoch [1], Sample [101], Average Loss: 0.9823
Training -- Epoch [1], Sample [102], Average Loss: 0.9753
Training -- Epoch [1], Sample [103], Average Loss: 0.9670
Training -- Epoch [1], Sample [104], Average Loss: 0.9600
Training -- Epoch [1], Sample [105], Average Loss: 0.9525
Training -- Epoch [1], Sample [106], Average Loss: 0.9453
Training -- Epoch [1], Sample [107], Average Loss: 0.9460
Training -- Epoch [1], Sample [108], Average Loss: 0.9381
Training -- Epoch [1], Sample [109], Average Loss: 0.9339
Training -- Epoch [1], Sample [110], Average Loss: 0.9291
Training -- Epoch [1], Sample [111], Average Loss: 0.9231
Training -- Epoch [1], Sample [112], Average Loss: 0.9160
Training -- Epoch [1], Sample [113], Average Loss: 0.9125
Training -- Epoch [1], Sample [114], Average Loss: 0.9048
Training -- Epoch [1], Sample [115], Average Loss: 0.8986
Training -- Epoch [1], Sample [116], Average Loss: 0.9001
Training -- Epoch [1], Sample [117], Average Loss: 0.9086
Training -- Epoch [1], Sample [118], Average Loss: 0.9024
Training -- Epoch [1], Sample [119], Average Loss: 0.8949
Training -- Epoch [1], Sample [120], Average Loss: 0.9102
Training -- Epoch [1], Sample [121], Average Loss: 0.9052
Training -- Epoch [1], Sample [122], Average Loss: 0.8998
Training -- Epoch [1], Sample [123], Average Loss: 0.8926
Training -- Epoch [1], Sample [124], Average Loss: 0.9030
Training -- Epoch [1], Sample [125], Average Loss: 0.9088
Training -- Epoch [1], Sample [126], Average Loss: 0.9106
Training -- Epoch [1], Sample [127], Average Loss: 0.9048
Training -- Epoch [1], Sample [128], Average Loss: 0.9025
Training -- Epoch [1], Sample [129], Average Loss: 0.8970
Training -- Epoch [1], Sample [130], Average Loss: 0.8918
Training -- Epoch [1], Sample [131], Average Loss: 0.8874
Training -- Epoch [1], Sample [132], Average Loss: 0.8816
Training -- Epoch [1], Sample [133], Average Loss: 0.8760
Training -- Epoch [1], Sample [134], Average Loss: 0.8792
Training -- Epoch [1], Sample [135], Average Loss: 0.8797
Training -- Epoch [1], Sample [136], Average Loss: 0.8770
Training -- Epoch [1], Sample [137], Average Loss: 0.8778
Training -- Epoch [1], Sample [138], Average Loss: 0.8723
Training -- Epoch [1], Sample [139], Average Loss: 0.8672
Training -- Epoch [1], Sample [140], Average Loss: 0.8656
Training -- Epoch [1], Sample [141], Average Loss: 0.8595
Training -- Epoch [1], Sample [142], Average Loss: 0.8540
Training -- Epoch [1], Sample [143], Average Loss: 0.8627
Training -- Epoch [1], Sample [144], Average Loss: 0.8573
Training -- Epoch [1], Sample [145], Average Loss: 0.8530
Training -- Epoch [1], Sample [146], Average Loss: 0.8500
Training -- Epoch [1], Sample [147], Average Loss: 0.8445
Training -- Epoch [1], Sample [148], Average Loss: 0.8400
Training -- Epoch [1], Sample [149], Average Loss: 0.8388
Training -- Epoch [1], Sample [150], Average Loss: 0.8525
Training -- Epoch [1], Sample [151], Average Loss: 0.8553
Training -- Epoch [1], Sample [152], Average Loss: 0.8501
Training -- Epoch [1], Sample [153], Average Loss: 0.8532
Training -- Epoch [1], Sample [154], Average Loss: 0.8485
Training -- Epoch [1], Sample [155], Average Loss: 0.8441
Training -- Epoch [1], Sample [156], Average Loss: 0.8446
Training -- Epoch [1], Sample [157], Average Loss: 0.8471
Training -- Epoch [1], Sample [158], Average Loss: 0.8476
Training -- Epoch [1], Sample [159], Average Loss: 0.8474
Training -- Epoch [1], Sample [160], Average Loss: 0.8442
Training -- Epoch [1], Sample [161], Average Loss: 0.8396
Training -- Epoch [1], Sample [162], Average Loss: 0.8405
Training -- Epoch [1], Sample [163], Average Loss: 0.8395
Training -- Epoch [1], Sample [164], Average Loss: 0.8400
Training -- Epoch [1], Sample [165], Average Loss: 0.8387
Training -- Epoch [1], Sample [166], Average Loss: 0.8436
Training -- Epoch [1], Sample [167], Average Loss: 0.8426
Training -- Epoch [1], Sample [168], Average Loss: 0.8409
Training -- Epoch [1], Sample [169], Average Loss: 0.8574
Training -- Epoch [1], Sample [170], Average Loss: 0.8746
Training -- Epoch [1], Sample [171], Average Loss: 0.8697
Training -- Epoch [1], Sample [172], Average Loss: 0.8661
Training -- Epoch [1], Sample [173], Average Loss: 0.8619
Training -- Epoch [1], Sample [174], Average Loss: 0.8579
Training -- Epoch [1], Sample [175], Average Loss: 0.8532
Training -- Epoch [1], Sample [176], Average Loss: 0.8495
Training -- Epoch [1], Sample [177], Average Loss: 0.8471
Training -- Epoch [1], Sample [178], Average Loss: 0.8628
Training -- Epoch [1], Sample [179], Average Loss: 0.8599
Training -- Epoch [1], Sample [180], Average Loss: 0.8562
Training -- Epoch [1], Sample [181], Average Loss: 0.8529
Training -- Epoch [1], Sample [182], Average Loss: 0.8520
Training -- Epoch [1], Sample [183], Average Loss: 0.8519
Training -- Epoch [1], Sample [184], Average Loss: 0.8478
Training -- Epoch [1], Sample [185], Average Loss: 0.8453
Training -- Epoch [1], Sample [186], Average Loss: 0.8423
Training -- Epoch [1], Sample [187], Average Loss: 0.8385
Training -- Epoch [1], Sample [188], Average Loss: 0.8343
Training -- Epoch [1], Sample [189], Average Loss: 0.8307
Training -- Epoch [1], Sample [190], Average Loss: 0.8269
Training -- Epoch [1], Sample [191], Average Loss: 0.8295
Training -- Epoch [1], Sample [192], Average Loss: 0.8253
Training -- Epoch [1], Sample [193], Average Loss: 0.8217
Training -- Epoch [1], Sample [194], Average Loss: 0.8192
Training -- Epoch [1], Sample [195], Average Loss: 0.8205
Training -- Epoch [1], Sample [196], Average Loss: 0.8191
Training -- Epoch [1], Sample [197], Average Loss: 0.8154
Training -- Epoch [1], Sample [198], Average Loss: 0.8118
Training -- Epoch [1], Sample [199], Average Loss: 0.8090
Training -- Epoch [1], Sample [200], Average Loss: 0.8057
Training -- Epoch [1], Sample [201], Average Loss: 0.8030
Training -- Epoch [1], Sample [202], Average Loss: 0.8057
Training -- Epoch [1], Sample [203], Average Loss: 0.8073
Training -- Epoch [1], Sample [204], Average Loss: 0.8085
Training -- Epoch [1], Sample [205], Average Loss: 0.8048
Training -- Epoch [1], Sample [206], Average Loss: 0.8010
Training -- Epoch [1], Sample [207], Average Loss: 0.8093
Training -- Epoch [1], Sample [208], Average Loss: 0.8058
Training -- Epoch [1], Sample [209], Average Loss: 0.8021
Training -- Epoch [1], Sample [210], Average Loss: 0.8015
Training -- Epoch [1], Sample [211], Average Loss: 0.8004
Training -- Epoch [1], Sample [212], Average Loss: 0.7969
Training -- Epoch [1], Sample [213], Average Loss: 0.7993
Training -- Epoch [1], Sample [214], Average Loss: 0.8065
Training -- Epoch [1], Sample [215], Average Loss: 0.8030
Training -- Epoch [1], Sample [216], Average Loss: 0.8040
Training -- Epoch [1], Sample [217], Average Loss: 0.8118
Training -- Epoch [1], Sample [218], Average Loss: 0.8094
Training -- Epoch [1], Sample [219], Average Loss: 0.8060
Training -- Epoch [1], Sample [220], Average Loss: 0.8041
Training -- Epoch [1], Sample [221], Average Loss: 0.8022
Training -- Epoch [1], Sample [222], Average Loss: 0.8003
Training -- Epoch [1], Sample [223], Average Loss: 0.7974
Training -- Epoch [1], Sample [224], Average Loss: 0.7961
Training -- Epoch [1], Sample [225], Average Loss: 0.8025
Training -- Epoch [1], Sample [226], Average Loss: 0.7997
Training -- Epoch [1], Sample [227], Average Loss: 0.8000
Training -- Epoch [1], Sample [228], Average Loss: 0.7966
Training -- Epoch [1], Sample [229], Average Loss: 0.7937
Training -- Epoch [1], Sample [230], Average Loss: 0.7903
Training -- Epoch [1], Sample [231], Average Loss: 0.7875
Training -- Epoch [1], Sample [232], Average Loss: 0.7888
Training -- Epoch [1], Sample [233], Average Loss: 0.7894
Training -- Epoch [1], Sample [234], Average Loss: 0.7896
Training -- Epoch [1], Sample [235], Average Loss: 0.7871
Training -- Epoch [1], Sample [236], Average Loss: 0.7880
Training -- Epoch [1], Sample [237], Average Loss: 0.7857
Training -- Epoch [1], Sample [238], Average Loss: 0.7841
Training -- Epoch [1], Sample [239], Average Loss: 0.7814
Training -- Epoch [1], Sample [240], Average Loss: 0.7805
Training -- Epoch [1], Sample [241], Average Loss: 0.7775
Training -- Epoch [1], Sample [242], Average Loss: 0.7773
Training -- Epoch [1], Sample [243], Average Loss: 0.7755
Training -- Epoch [1], Sample [244], Average Loss: 0.7734
Training -- Epoch [1], Sample [245], Average Loss: 0.7704
Training -- Epoch [1], Sample [246], Average Loss: 0.7678
Training -- Epoch [1], Sample [247], Average Loss: 0.7650
Training -- Epoch [1], Sample [248], Average Loss: 0.7624
Training -- Epoch [1], Sample [249], Average Loss: 0.7659
Training -- Epoch [1], Sample [250], Average Loss: 0.7675
Training -- Epoch [1], Sample [251], Average Loss: 0.7758
Training -- Epoch [1], Sample [252], Average Loss: 0.7741
Training -- Epoch [1], Sample [253], Average Loss: 0.7713
Training -- Epoch [1], Sample [254], Average Loss: 0.7683
Training -- Epoch [1], Sample [255], Average Loss: 0.7674
Training -- Epoch [1], Sample [256], Average Loss: 0.7647
Training -- Epoch [1], Sample [257], Average Loss: 0.7632
Training -- Epoch [1], Sample [258], Average Loss: 0.7606
Training -- Epoch [1], Sample [259], Average Loss: 0.7591
Training -- Epoch [1], Sample [260], Average Loss: 0.7592
Training -- Epoch [1], Sample [261], Average Loss: 0.7596
Training -- Epoch [1], Sample [262], Average Loss: 0.7573
Training -- Epoch [1], Sample [263], Average Loss: 0.7581
Training -- Epoch [1], Sample [264], Average Loss: 0.7557
Training -- Epoch [1], Sample [265], Average Loss: 0.7678
Training -- Epoch [1], Sample [266], Average Loss: 0.7652
Training -- Epoch [1], Sample [267], Average Loss: 0.7642
Training -- Epoch [1], Sample [268], Average Loss: 0.7623
Training -- Epoch [1], Sample [269], Average Loss: 0.7623
Training -- Epoch [1], Sample [270], Average Loss: 0.7625
Training -- Epoch [1], Sample [271], Average Loss: 0.7721
Training -- Epoch [1], Sample [272], Average Loss: 0.7713
Training -- Epoch [1], Sample [273], Average Loss: 0.7686
Training -- Epoch [1], Sample [274], Average Loss: 0.7661
Training -- Epoch [1], Sample [275], Average Loss: 0.7652
Training -- Epoch [1], Sample [276], Average Loss: 0.7632
Training -- Epoch [1], Sample [277], Average Loss: 0.7610
Training -- Epoch [1], Sample [278], Average Loss: 0.7601
Training -- Epoch [1], Sample [279], Average Loss: 0.7587
Training -- Epoch [1], Sample [280], Average Loss: 0.7562
Training -- Epoch [1], Sample [281], Average Loss: 0.7788
Training -- Epoch [1], Sample [282], Average Loss: 0.7769
Training -- Epoch [1], Sample [283], Average Loss: 0.7747
Training -- Epoch [1], Sample [284], Average Loss: 0.7720
Training -- Epoch [1], Sample [285], Average Loss: 0.7709
Training -- Epoch [1], Sample [286], Average Loss: 0.7706
Training -- Epoch [1], Sample [287], Average Loss: 0.7705
Training -- Epoch [1], Sample [288], Average Loss: 0.7680
Training -- Epoch [1], Sample [289], Average Loss: 0.7801
Training -- Epoch [1], Sample [290], Average Loss: 0.7824
Training -- Epoch [1], Sample [291], Average Loss: 0.7823
Training -- Epoch [1], Sample [292], Average Loss: 0.7829
Training -- Epoch [1], Sample [293], Average Loss: 0.7898
Training -- Epoch [1], Sample [294], Average Loss: 0.7882
Training -- Epoch [1], Sample [295], Average Loss: 0.7858
Training -- Epoch [1], Sample [296], Average Loss: 0.7836
Training -- Epoch [1], Sample [297], Average Loss: 0.7832
Training -- Epoch [1], Sample [298], Average Loss: 0.7870
Training -- Epoch [1], Sample [299], Average Loss: 0.7849
Training -- Epoch [1], Sample [300], Average Loss: 0.8015
Training -- Epoch [1], Sample [301], Average Loss: 0.7991
Training -- Epoch [1], Sample [302], Average Loss: 0.7968
Training -- Epoch [1], Sample [303], Average Loss: 0.7952
Training -- Epoch [1], Sample [304], Average Loss: 0.7938
Training -- Epoch [1], Sample [305], Average Loss: 0.7927
Training -- Epoch [1], Sample [306], Average Loss: 0.7903
Training -- Epoch [1], Sample [307], Average Loss: 0.7940
Training -- Epoch [1], Sample [308], Average Loss: 0.7933
Training -- Epoch [1], Sample [309], Average Loss: 0.7908
Training -- Epoch [1], Sample [310], Average Loss: 0.7946
Training -- Epoch [1], Sample [311], Average Loss: 0.7937
Training -- Epoch [1], Sample [312], Average Loss: 0.7915
Training -- Epoch [1], Sample [313], Average Loss: 0.7890
Training -- Epoch [1], Sample [314], Average Loss: 0.7871
Training -- Epoch [1], Sample [315], Average Loss: 0.7861
Training -- Epoch [1], Sample [316], Average Loss: 0.7840
Training -- Epoch [1], Sample [317], Average Loss: 0.7816
Training -- Epoch [1], Sample [318], Average Loss: 0.7797
Training -- Epoch [1], Sample [319], Average Loss: 0.7777
Training -- Epoch [1], Sample [320], Average Loss: 0.7755
Training -- Epoch [1], Sample [321], Average Loss: 0.7783
Training -- Epoch [1], Sample [322], Average Loss: 0.7816
Training -- Epoch [1], Sample [323], Average Loss: 0.7809
Training -- Epoch [1], Sample [324], Average Loss: 0.7793
Training -- Epoch [1], Sample [325], Average Loss: 0.7807
Training -- Epoch [1], Sample [326], Average Loss: 0.7795
Training -- Epoch [1], Sample [327], Average Loss: 0.7792
Training -- Epoch [1], Sample [328], Average Loss: 0.7786
Training -- Epoch [1], Sample [329], Average Loss: 0.7775
Training -- Epoch [1], Sample [330], Average Loss: 0.7756
Training -- Epoch [1], Sample [331], Average Loss: 0.7740
Training -- Epoch [1], Sample [332], Average Loss: 0.7741
Training -- Epoch [1], Sample [333], Average Loss: 0.7722
Training -- Epoch [1], Sample [334], Average Loss: 0.7701
Training -- Epoch [1], Sample [335], Average Loss: 0.7679
Training -- Epoch [1], Sample [336], Average Loss: 0.7670
Training -- Epoch [1], Sample [337], Average Loss: 0.7666
Training -- Epoch [1], Sample [338], Average Loss: 0.7646
Training -- Epoch [1], Sample [339], Average Loss: 0.7706
Training -- Epoch [1], Sample [340], Average Loss: 0.7685
Training -- Epoch [1], Sample [341], Average Loss: 0.7676
Training -- Epoch [1], Sample [342], Average Loss: 0.7656
Training -- Epoch [1], Sample [343], Average Loss: 0.7639
Training -- Epoch [1], Sample [344], Average Loss: 0.7617
Training -- Epoch [1], Sample [345], Average Loss: 0.7628
Training -- Epoch [1], Sample [346], Average Loss: 0.7660
Training -- Epoch [1], Sample [347], Average Loss: 0.7643
Training -- Epoch [1], Sample [348], Average Loss: 0.7627
Training -- Epoch [1], Sample [349], Average Loss: 0.7616
Training -- Epoch [1], Sample [350], Average Loss: 0.7604
Training -- Epoch [1], Sample [351], Average Loss: 0.7584
Training -- Epoch [1], Sample [352], Average Loss: 0.7577
Training -- Epoch [1], Sample [353], Average Loss: 0.7580
Training -- Epoch [1], Sample [354], Average Loss: 0.7564
Training -- Epoch [1], Sample [355], Average Loss: 0.7566
Training -- Epoch [1], Sample [356], Average Loss: 0.7586
Training -- Epoch [1], Sample [357], Average Loss: 0.7615
Training -- Epoch [1], Sample [358], Average Loss: 0.7620
Training -- Epoch [1], Sample [359], Average Loss: 0.7614
Training -- Epoch [1], Sample [360], Average Loss: 0.7600
Training -- Epoch [1], Sample [361], Average Loss: 0.7582
Training -- Epoch [1], Sample [362], Average Loss: 0.7578
Training -- Epoch [1], Sample [363], Average Loss: 0.7560
Training -- Epoch [1], Sample [364], Average Loss: 0.7546
Training -- Epoch [1], Sample [365], Average Loss: 0.7526
Training -- Epoch [1], Sample [366], Average Loss: 0.7519
Training -- Epoch [1], Sample [367], Average Loss: 0.7542
Training -- Epoch [1], Sample [368], Average Loss: 0.7524
Training -- Epoch [1], Sample [369], Average Loss: 0.7506
Training -- Epoch [1], Sample [370], Average Loss: 0.7490
Training -- Epoch [1], Sample [371], Average Loss: 0.7489
Training -- Epoch [1], Sample [372], Average Loss: 0.7498
Training -- Epoch [1], Sample [373], Average Loss: 0.7483
Training -- Epoch [1], Sample [374], Average Loss: 0.7593
Training -- Epoch [1], Sample [375], Average Loss: 0.7578
Training -- Epoch [1], Sample [376], Average Loss: 0.7595
Training -- Epoch [1], Sample [377], Average Loss: 0.7621
Training -- Epoch [1], Sample [378], Average Loss: 0.7611
Training -- Epoch [1], Sample [379], Average Loss: 0.7594
Training -- Epoch [1], Sample [380], Average Loss: 0.7577
Training -- Epoch [1], Sample [381], Average Loss: 0.7565
Training -- Epoch [1], Sample [382], Average Loss: 0.7573
Training -- Epoch [1], Sample [383], Average Loss: 0.7564
Training -- Epoch [1], Sample [384], Average Loss: 0.7650
Training -- Epoch [1], Sample [385], Average Loss: 0.7635
Training -- Epoch [1], Sample [386], Average Loss: 0.7661
Training -- Epoch [1], Sample [387], Average Loss: 0.7690
Training -- Epoch [1], Sample [388], Average Loss: 0.7705
Training -- Epoch [1], Sample [389], Average Loss: 0.7696
Training -- Epoch [1], Sample [390], Average Loss: 0.7680
Training -- Epoch [1], Sample [391], Average Loss: 0.7666
Training -- Epoch [1], Sample [392], Average Loss: 0.7698
Training -- Epoch [1], Sample [393], Average Loss: 0.7684
Training -- Epoch [1], Sample [394], Average Loss: 0.7683
Training -- Epoch [1], Sample [395], Average Loss: 0.7684
Training -- Epoch [1], Sample [396], Average Loss: 0.7678
Training -- Epoch [1], Sample [397], Average Loss: 0.7746
Training -- Epoch [1], Sample [398], Average Loss: 0.7735
Training -- Epoch [1], Sample [399], Average Loss: 0.7779
Training -- Epoch [1], Sample [400], Average Loss: 0.7766
Training -- Epoch [1], Sample [401], Average Loss: 0.7753
Training -- Epoch [1], Sample [402], Average Loss: 0.7743
Training -- Epoch [1], Sample [403], Average Loss: 0.7725
Training -- Epoch [1], Sample [404], Average Loss: 0.7715
Training -- Epoch [1], Sample [405], Average Loss: 0.7716
Training -- Epoch [1], Sample [406], Average Loss: 0.7699
Training -- Epoch [1], Sample [407], Average Loss: 0.7726
Training -- Epoch [1], Sample [408], Average Loss: 0.7718
Training -- Epoch [1], Sample [409], Average Loss: 0.7716
Training -- Epoch [1], Sample [410], Average Loss: 0.7737
Training -- Epoch [1], Sample [411], Average Loss: 0.7723
Training -- Epoch [1], Sample [412], Average Loss: 0.7708
Training -- Epoch [1], Sample [413], Average Loss: 0.7693
Training -- Epoch [1], Sample [414], Average Loss: 0.7687
Training -- Epoch [1], Sample [415], Average Loss: 0.7674
Training -- Epoch [1], Sample [416], Average Loss: 0.7656
Training -- Epoch [1], Sample [417], Average Loss: 0.7656
Training -- Epoch [1], Sample [418], Average Loss: 0.7699
Training -- Epoch [1], Sample [419], Average Loss: 0.7691
Training -- Epoch [1], Sample [420], Average Loss: 0.7707
Training -- Epoch [1], Sample [421], Average Loss: 0.7697
Training -- Epoch [1], Sample [422], Average Loss: 0.7682
Training -- Epoch [1], Sample [423], Average Loss: 0.7666
Training -- Epoch [1], Sample [424], Average Loss: 0.7675
Training -- Epoch [1], Sample [425], Average Loss: 0.7662
Training -- Epoch [1], Sample [426], Average Loss: 0.7656
Training -- Epoch [1], Sample [427], Average Loss: 0.7647
Training -- Epoch [1], Sample [428], Average Loss: 0.7630
Training -- Epoch [1], Sample [429], Average Loss: 0.7632
Training -- Epoch [1], Sample [430], Average Loss: 0.7690
Training -- Epoch [1], Sample [431], Average Loss: 0.7681
Training -- Epoch [1], Sample [432], Average Loss: 0.7674
Training -- Epoch [1], Sample [433], Average Loss: 0.7659
Training -- Epoch [1], Sample [434], Average Loss: 0.7658
Training -- Epoch [1], Sample [435], Average Loss: 0.7642
Training -- Epoch [1], Sample [436], Average Loss: 0.7627
Training -- Epoch [1], Sample [437], Average Loss: 0.7631
Training -- Epoch [1], Sample [438], Average Loss: 0.7617
Training -- Epoch [1], Sample [439], Average Loss: 0.7614
Training -- Epoch [1], Sample [440], Average Loss: 0.7599
Training -- Epoch [1], Sample [441], Average Loss: 0.7592
Training -- Epoch [1], Sample [442], Average Loss: 0.7598
Training -- Epoch [1], Sample [443], Average Loss: 0.7587
Training -- Epoch [1], Sample [444], Average Loss: 0.7597
Training -- Epoch [1], Sample [445], Average Loss: 0.7605
Training -- Epoch [1], Sample [446], Average Loss: 0.7589
Training -- Epoch [1], Sample [447], Average Loss: 0.7588
Training -- Epoch [1], Sample [448], Average Loss: 0.7589
Training -- Epoch [1], Sample [449], Average Loss: 0.7584
Training -- Epoch [1], Sample [450], Average Loss: 0.7582
Training -- Epoch [1], Sample [451], Average Loss: 0.7585
Training -- Epoch [1], Sample [452], Average Loss: 0.7569
Training -- Epoch [1], Sample [453], Average Loss: 0.7556
Training -- Epoch [1], Sample [454], Average Loss: 0.7557
Training -- Epoch [1], Sample [455], Average Loss: 0.7548
Training -- Epoch [1], Sample [456], Average Loss: 0.7532
Training -- Epoch [1], Sample [457], Average Loss: 0.7524
Training -- Epoch [1], Sample [458], Average Loss: 0.7548
Training -- Epoch [1], Sample [459], Average Loss: 0.7559
Training -- Epoch [1], Sample [460], Average Loss: 0.7554
Training -- Epoch [1], Sample [461], Average Loss: 0.7602
Training -- Epoch [1], Sample [462], Average Loss: 0.7594
Training -- Epoch [1], Sample [463], Average Loss: 0.7584
Training -- Epoch [1], Sample [464], Average Loss: 0.7570
Training -- Epoch [1], Sample [465], Average Loss: 0.7559
Training -- Epoch [1], Sample [466], Average Loss: 0.7546
Training -- Epoch [1], Sample [467], Average Loss: 0.7532
Training -- Epoch [1], Sample [468], Average Loss: 0.7517
Training -- Epoch [1], Sample [469], Average Loss: 0.7562
Training -- Epoch [1], Sample [470], Average Loss: 0.7569
Training -- Epoch [1], Sample [471], Average Loss: 0.7554
Training -- Epoch [1], Sample [472], Average Loss: 0.7566
Training -- Epoch [1], Sample [473], Average Loss: 0.7552
Training -- Epoch [1], Sample [474], Average Loss: 0.7545
Training -- Epoch [1], Sample [475], Average Loss: 0.7533
Training -- Epoch [1], Sample [476], Average Loss: 0.7518
Training -- Epoch [1], Sample [477], Average Loss: 0.7503
Training -- Epoch [1], Sample [478], Average Loss: 0.7488
Training -- Epoch [1], Sample [479], Average Loss: 0.7475
Training -- Epoch [1], Sample [480], Average Loss: 0.7467
Training -- Epoch [1], Sample [481], Average Loss: 0.7452
Training -- Epoch [1], Sample [482], Average Loss: 0.7443
Training -- Epoch [1], Sample [483], Average Loss: 0.7432
Training -- Epoch [1], Sample [484], Average Loss: 0.7424
Training -- Epoch [1], Sample [485], Average Loss: 0.7423
Training -- Epoch [1], Sample [486], Average Loss: 0.7409
Training -- Epoch [1], Sample [487], Average Loss: 0.7434
Training -- Epoch [1], Sample [488], Average Loss: 0.7424
Training -- Epoch [1], Sample [489], Average Loss: 0.7411
Training -- Epoch [1], Sample [490], Average Loss: 0.7400
Training -- Epoch [1], Sample [491], Average Loss: 0.7385
Training -- Epoch [1], Sample [492], Average Loss: 0.7372
Training -- Epoch [1], Sample [493], Average Loss: 0.7360
Training -- Epoch [1], Sample [494], Average Loss: 0.7347
Training -- Epoch [1], Sample [495], Average Loss: 0.7352
Training -- Epoch [1], Sample [496], Average Loss: 0.7339
Training -- Epoch [1], Sample [497], Average Loss: 0.7325
Training -- Epoch [1], Sample [498], Average Loss: 0.7312
Training -- Epoch [1], Sample [499], Average Loss: 0.7318
Training -- Epoch [1], Sample [500], Average Loss: 0.7303
Training -- Epoch [1], Sample [501], Average Loss: 0.7303
Training -- Epoch [1], Sample [502], Average Loss: 0.7296
Training -- Epoch [1], Sample [503], Average Loss: 0.7286
Training -- Epoch [1], Sample [504], Average Loss: 0.7302
Training -- Epoch [1], Sample [505], Average Loss: 0.7289
Training -- Epoch [1], Sample [506], Average Loss: 0.7316
Training -- Epoch [1], Sample [507], Average Loss: 0.7302
Training -- Epoch [1], Sample [508], Average Loss: 0.7304
Training -- Epoch [1], Sample [509], Average Loss: 0.7293
Training -- Epoch [1], Sample [510], Average Loss: 0.7284
Training -- Epoch [1], Sample [511], Average Loss: 0.7300
Training -- Epoch [1], Sample [512], Average Loss: 0.7297
Training -- Epoch [1], Sample [513], Average Loss: 0.7283
Training -- Epoch [1], Sample [514], Average Loss: 0.7304
Training -- Epoch [1], Sample [515], Average Loss: 0.7293
Training -- Epoch [1], Sample [516], Average Loss: 0.7287
Training -- Epoch [1], Sample [517], Average Loss: 0.7289
Training -- Epoch [1], Sample [518], Average Loss: 0.7276
Training -- Epoch [1], Sample [519], Average Loss: 0.7263
Training -- Epoch [1], Sample [520], Average Loss: 0.7254
Training -- Epoch [1], Sample [521], Average Loss: 0.7270
Training -- Epoch [1], Sample [522], Average Loss: 0.7263
Training -- Epoch [1], Sample [523], Average Loss: 0.7251
Training -- Epoch [1], Sample [524], Average Loss: 0.7238
Training -- Epoch [1], Sample [525], Average Loss: 0.7225
Training -- Epoch [1], Sample [526], Average Loss: 0.7217
Training -- Epoch [1], Sample [527], Average Loss: 0.7205
Training -- Epoch [1], Sample [528], Average Loss: 0.7192
Training -- Epoch [1], Sample [529], Average Loss: 0.7181
Training -- Epoch [1], Sample [530], Average Loss: 0.7169
Training -- Epoch [1], Sample [531], Average Loss: 0.7161
Training -- Epoch [1], Sample [532], Average Loss: 0.7156
Training -- Epoch [1], Sample [533], Average Loss: 0.7146
Training -- Epoch [1], Sample [534], Average Loss: 0.7140
Training -- Epoch [1], Sample [535], Average Loss: 0.7135
Training -- Epoch [1], Sample [536], Average Loss: 0.7122
Training -- Epoch [1], Sample [537], Average Loss: 0.7187
Training -- Epoch [1], Sample [538], Average Loss: 0.7177
Training -- Epoch [1], Sample [539], Average Loss: 0.7175
Training -- Epoch [1], Sample [540], Average Loss: 0.7164
Training -- Epoch [1], Sample [541], Average Loss: 0.7173
Training -- Epoch [1], Sample [542], Average Loss: 0.7172
Training -- Epoch [1], Sample [543], Average Loss: 0.7159
Training -- Epoch [1], Sample [544], Average Loss: 0.7162
Training -- Epoch [1], Sample [545], Average Loss: 0.7167
Training -- Epoch [1], Sample [546], Average Loss: 0.7160
Training -- Epoch [1], Sample [547], Average Loss: 0.7210
Training -- Epoch [1], Sample [548], Average Loss: 0.7198
Training -- Epoch [1], Sample [549], Average Loss: 0.7199
Training -- Epoch [1], Sample [550], Average Loss: 0.7195
Training -- Epoch [1], Sample [551], Average Loss: 0.7190
Training -- Epoch [1], Sample [552], Average Loss: 0.7179
Training -- Epoch [1], Sample [553], Average Loss: 0.7170
Training -- Epoch [1], Sample [554], Average Loss: 0.7204
Training -- Epoch [1], Sample [555], Average Loss: 0.7199
Training -- Epoch [1], Sample [556], Average Loss: 0.7216
Training -- Epoch [1], Sample [557], Average Loss: 0.7206
Training -- Epoch [1], Sample [558], Average Loss: 0.7193
Training -- Epoch [1], Sample [559], Average Loss: 0.7191
Training -- Epoch [1], Sample [560], Average Loss: 0.7181
Training -- Epoch [1], Sample [561], Average Loss: 0.7191
Training -- Epoch [1], Sample [562], Average Loss: 0.7207
Training -- Epoch [1], Sample [563], Average Loss: 0.7195
Training -- Epoch [1], Sample [564], Average Loss: 0.7216
Training -- Epoch [1], Sample [565], Average Loss: 0.7207
Training -- Epoch [1], Sample [566], Average Loss: 0.7197
Training -- Epoch [1], Sample [567], Average Loss: 0.7196
Training -- Epoch [1], Sample [568], Average Loss: 0.7191
Training -- Epoch [1], Sample [569], Average Loss: 0.7179
Training -- Epoch [1], Sample [570], Average Loss: 0.7170
Training -- Epoch [1], Sample [571], Average Loss: 0.7177
Training -- Epoch [1], Sample [572], Average Loss: 0.7166
Training -- Epoch [1], Sample [573], Average Loss: 0.7159
Training -- Epoch [1], Sample [574], Average Loss: 0.7149
Training -- Epoch [1], Sample [575], Average Loss: 0.7138
Training -- Epoch [1], Sample [576], Average Loss: 0.7132
Training -- Epoch [1], Sample [577], Average Loss: 0.7122
Training -- Epoch [1], Sample [578], Average Loss: 0.7175
Training -- Epoch [1], Sample [579], Average Loss: 0.7190
Training -- Epoch [1], Sample [580], Average Loss: 0.7183
Training -- Epoch [1], Sample [581], Average Loss: 0.7176
Training -- Epoch [1], Sample [582], Average Loss: 0.7165
Training -- Epoch [1], Sample [583], Average Loss: 0.7156
Training -- Epoch [1], Sample [584], Average Loss: 0.7158
Training -- Epoch [1], Sample [585], Average Loss: 0.7165
Training -- Epoch [1], Sample [586], Average Loss: 0.7156
Training -- Epoch [1], Sample [587], Average Loss: 0.7149
Training -- Epoch [1], Sample [588], Average Loss: 0.7137
Training -- Epoch [1], Sample [589], Average Loss: 0.7139
Training -- Epoch [1], Sample [590], Average Loss: 0.7132
Training -- Epoch [1], Sample [591], Average Loss: 0.7133
Training -- Epoch [1], Sample [592], Average Loss: 0.7122
Training -- Epoch [1], Sample [593], Average Loss: 0.7119
Training -- Epoch [1], Sample [594], Average Loss: 0.7109
Training -- Epoch [1], Sample [595], Average Loss: 0.7098
Training -- Epoch [1], Sample [596], Average Loss: 0.7104
Training -- Epoch [1], Sample [597], Average Loss: 0.7103
Training -- Epoch [1], Sample [598], Average Loss: 0.7098
Training -- Epoch [1], Sample [599], Average Loss: 0.7087
Training -- Epoch [1], Sample [600], Average Loss: 0.7077
Training -- Epoch [1], Sample [601], Average Loss: 0.7073
Training -- Epoch [1], Sample [602], Average Loss: 0.7062
Training -- Epoch [1], Sample [603], Average Loss: 0.7051
Training -- Epoch [1], Sample [604], Average Loss: 0.7053
Training -- Epoch [1], Sample [605], Average Loss: 0.7062
Training -- Epoch [1], Sample [606], Average Loss: 0.7054
Training -- Epoch [1], Sample [607], Average Loss: 0.7044
Training -- Epoch [1], Sample [608], Average Loss: 0.7070
Training -- Epoch [1], Sample [609], Average Loss: 0.7067
Training -- Epoch [1], Sample [610], Average Loss: 0.7064
Training -- Epoch [1], Sample [611], Average Loss: 0.7058
Training -- Epoch [1], Sample [612], Average Loss: 0.7049
Training -- Epoch [1], Sample [613], Average Loss: 0.7040
Training -- Epoch [1], Sample [614], Average Loss: 0.7029
Training -- Epoch [1], Sample [615], Average Loss: 0.7027
Training -- Epoch [1], Sample [616], Average Loss: 0.7018
Training -- Epoch [1], Sample [617], Average Loss: 0.7013
Training -- Epoch [1], Sample [618], Average Loss: 0.7003
Training -- Epoch [1], Sample [619], Average Loss: 0.7002
Training -- Epoch [1], Sample [620], Average Loss: 0.7018
Training -- Epoch [1], Sample [621], Average Loss: 0.7030
Training -- Epoch [1], Sample [622], Average Loss: 0.7032
Training -- Epoch [1], Sample [623], Average Loss: 0.7022
Training -- Epoch [1], Sample [624], Average Loss: 0.7016
Training -- Epoch [1], Sample [625], Average Loss: 0.7006
Training -- Epoch [1], Sample [626], Average Loss: 0.7012
Training -- Epoch [1], Sample [627], Average Loss: 0.7004
Training -- Epoch [1], Sample [628], Average Loss: 0.7009
Training -- Epoch [1], Sample [629], Average Loss: 0.7001
Training -- Epoch [1], Sample [630], Average Loss: 0.6992
Training -- Epoch [1], Sample [631], Average Loss: 0.6984
Training -- Epoch [1], Sample [632], Average Loss: 0.7002
Training -- Epoch [1], Sample [633], Average Loss: 0.6991
Training -- Epoch [1], Sample [634], Average Loss: 0.6989
Training -- Epoch [1], Sample [635], Average Loss: 0.6984
Training -- Epoch [1], Sample [636], Average Loss: 0.6975
Training -- Epoch [1], Sample [637], Average Loss: 0.6966
Training -- Epoch [1], Sample [638], Average Loss: 0.6956
Training -- Epoch [1], Sample [639], Average Loss: 0.6948
Training -- Epoch [1], Sample [640], Average Loss: 0.6943
Training -- Epoch [1], Sample [641], Average Loss: 0.6935
Training -- Epoch [1], Sample [642], Average Loss: 0.6924
Training -- Epoch [1], Sample [643], Average Loss: 0.6914
Training -- Epoch [1], Sample [644], Average Loss: 0.6910
Training -- Epoch [1], Sample [645], Average Loss: 0.6913
Training -- Epoch [1], Sample [646], Average Loss: 0.6912
Training -- Epoch [1], Sample [647], Average Loss: 0.6903
Training -- Epoch [1], Sample [648], Average Loss: 0.6923
Training -- Epoch [1], Sample [649], Average Loss: 0.6920
Training -- Epoch [1], Sample [650], Average Loss: 0.6979
Training -- Epoch [1], Sample [651], Average Loss: 0.6983
Training -- Epoch [1], Sample [652], Average Loss: 0.6978
Training -- Epoch [1], Sample [653], Average Loss: 0.6970
Training -- Epoch [1], Sample [654], Average Loss: 0.7038
Training -- Epoch [1], Sample [655], Average Loss: 0.7031
Training -- Epoch [1], Sample [656], Average Loss: 0.7023
Training -- Epoch [1], Sample [657], Average Loss: 0.7013
Training -- Epoch [1], Sample [658], Average Loss: 0.7005
Training -- Epoch [1], Sample [659], Average Loss: 0.7004
Training -- Epoch [1], Sample [660], Average Loss: 0.7000
Training -- Epoch [1], Sample [661], Average Loss: 0.6991
Training -- Epoch [1], Sample [662], Average Loss: 0.6982
Training -- Epoch [1], Sample [663], Average Loss: 0.6975
Training -- Epoch [1], Sample [664], Average Loss: 0.6969
Training -- Epoch [1], Sample [665], Average Loss: 0.6962
Training -- Epoch [1], Sample [666], Average Loss: 0.6956
Training -- Epoch [1], Sample [667], Average Loss: 0.6973
Training -- Epoch [1], Sample [668], Average Loss: 0.6963
Training -- Epoch [1], Sample [669], Average Loss: 0.6957
Training -- Epoch [1], Sample [670], Average Loss: 0.6949
Training -- Epoch [1], Sample [671], Average Loss: 0.6970
Training -- Epoch [1], Sample [672], Average Loss: 0.6964
Training -- Epoch [1], Sample [673], Average Loss: 0.6954
Training -- Epoch [1], Sample [674], Average Loss: 0.6960
Training -- Epoch [1], Sample [675], Average Loss: 0.6952
Training -- Epoch [1], Sample [676], Average Loss: 0.6948
Training -- Epoch [1], Sample [677], Average Loss: 0.6940
Training -- Epoch [1], Sample [678], Average Loss: 0.6936
Training -- Epoch [1], Sample [679], Average Loss: 0.6930
Training -- Epoch [1], Sample [680], Average Loss: 0.6926
Training -- Epoch [1], Sample [681], Average Loss: 0.6924
Training -- Epoch [1], Sample [682], Average Loss: 0.6922
Training -- Epoch [1], Sample [683], Average Loss: 0.6917
Training -- Epoch [1], Sample [684], Average Loss: 0.6910
Training -- Epoch [1], Sample [685], Average Loss: 0.6917
Training -- Epoch [1], Sample [686], Average Loss: 0.6908
Training -- Epoch [1], Sample [687], Average Loss: 0.6900
Training -- Epoch [1], Sample [688], Average Loss: 0.6893
Training -- Epoch [1], Sample [689], Average Loss: 0.6885
Training -- Epoch [1], Sample [690], Average Loss: 0.6882
Training -- Epoch [1], Sample [691], Average Loss: 0.6902
Training -- Epoch [1], Sample [692], Average Loss: 0.6896
Training -- Epoch [1], Sample [693], Average Loss: 0.6903
Training -- Epoch [1], Sample [694], Average Loss: 0.6894
Training -- Epoch [1], Sample [695], Average Loss: 0.6887
Training -- Epoch [1], Sample [696], Average Loss: 0.6880
Training -- Epoch [1], Sample [697], Average Loss: 0.6887
Training -- Epoch [1], Sample [698], Average Loss: 0.6904
Training -- Epoch [1], Sample [699], Average Loss: 0.6900
Training -- Epoch [1], Sample [700], Average Loss: 0.6897
Training -- Epoch [1], Sample [701], Average Loss: 0.6912
Training -- Epoch [1], Sample [702], Average Loss: 0.6905
Training -- Epoch [1], Sample [703], Average Loss: 0.6897
Training -- Epoch [1], Sample [704], Average Loss: 0.6899
Training -- Epoch [1], Sample [705], Average Loss: 0.6893
Training -- Epoch [1], Sample [706], Average Loss: 0.6883
Training -- Epoch [1], Sample [707], Average Loss: 0.6882
Training -- Epoch [1], Sample [708], Average Loss: 0.6874
Training -- Epoch [1], Sample [709], Average Loss: 0.6866
Training -- Epoch [1], Sample [710], Average Loss: 0.6888
Training -- Epoch [1], Sample [711], Average Loss: 0.6880
Training -- Epoch [1], Sample [712], Average Loss: 0.6872
Training -- Epoch [1], Sample [713], Average Loss: 0.6864
Training -- Epoch [1], Sample [714], Average Loss: 0.6867
Training -- Epoch [1], Sample [715], Average Loss: 0.6863
Training -- Epoch [1], Sample [716], Average Loss: 0.6854
Training -- Epoch [1], Sample [717], Average Loss: 0.6846
Training -- Epoch [1], Sample [718], Average Loss: 0.6852
Training -- Epoch [1], Sample [719], Average Loss: 0.6847
Training -- Epoch [1], Sample [720], Average Loss: 0.6860
Training -- Epoch [1], Sample [721], Average Loss: 0.6853
Training -- Epoch [1], Sample [722], Average Loss: 0.6843
Training -- Epoch [1], Sample [723], Average Loss: 0.6834
Training -- Epoch [1], Sample [724], Average Loss: 0.6851
Training -- Epoch [1], Sample [725], Average Loss: 0.6881
Training -- Epoch [1], Sample [726], Average Loss: 0.6872
Training -- Epoch [1], Sample [727], Average Loss: 0.6864
Training -- Epoch [1], Sample [728], Average Loss: 0.6891
Training -- Epoch [1], Sample [729], Average Loss: 0.6882
Training -- Epoch [1], Sample [730], Average Loss: 0.6884
Training -- Epoch [1], Sample [731], Average Loss: 0.6888
Training -- Epoch [1], Sample [732], Average Loss: 0.6882
Training -- Epoch [1], Sample [733], Average Loss: 0.6875
Training -- Epoch [1], Sample [734], Average Loss: 0.6911
Training -- Epoch [1], Sample [735], Average Loss: 0.6902
Training -- Epoch [1], Sample [736], Average Loss: 0.6905
Training -- Epoch [1], Sample [737], Average Loss: 0.6899
Training -- Epoch [1], Sample [738], Average Loss: 0.6891
Training -- Epoch [1], Sample [739], Average Loss: 0.6882
Training -- Epoch [1], Sample [740], Average Loss: 0.6885
Training -- Epoch [1], Sample [741], Average Loss: 0.6879
Training -- Epoch [1], Sample [742], Average Loss: 0.6877
Training -- Epoch [1], Sample [743], Average Loss: 0.6874
Training -- Epoch [1], Sample [744], Average Loss: 0.6868
Training -- Epoch [1], Sample [745], Average Loss: 0.6859
Training -- Epoch [1], Sample [746], Average Loss: 0.6853
Training -- Epoch [1], Sample [747], Average Loss: 0.6882
Training -- Epoch [1], Sample [748], Average Loss: 0.6879
Training -- Epoch [1], Sample [749], Average Loss: 0.6876
Training -- Epoch [1], Sample [750], Average Loss: 0.6872
Training -- Epoch [1], Sample [751], Average Loss: 0.6867
Training -- Epoch [1], Sample [752], Average Loss: 0.6871
Training -- Epoch [1], Sample [753], Average Loss: 0.6863
Training -- Epoch [1], Sample [754], Average Loss: 0.6867
Training -- Epoch [1], Sample [755], Average Loss: 0.6865
Training -- Epoch [1], Sample [756], Average Loss: 0.6857
Training -- Epoch [1], Sample [757], Average Loss: 0.6852
Training -- Epoch [1], Sample [758], Average Loss: 0.6845
Training -- Epoch [1], Sample [759], Average Loss: 0.6841
Training -- Epoch [1], Sample [760], Average Loss: 0.6846
Training -- Epoch [1], Sample [761], Average Loss: 0.6842
Training -- Epoch [1], Sample [762], Average Loss: 0.6835
Training -- Epoch [1], Sample [763], Average Loss: 0.6833
Training -- Epoch [1], Sample [764], Average Loss: 0.6826
Training -- Epoch [1], Sample [765], Average Loss: 0.6817
Training -- Epoch [1], Sample [766], Average Loss: 0.6811
Training -- Epoch [1], Sample [767], Average Loss: 0.6804
Training -- Epoch [1], Sample [768], Average Loss: 0.6795
Training -- Epoch [1], Sample [769], Average Loss: 0.6787
Training -- Epoch [1], Sample [770], Average Loss: 0.6779
Training -- Epoch [1], Sample [771], Average Loss: 0.6796
Training -- Epoch [1], Sample [772], Average Loss: 0.6793
Training -- Epoch [1], Sample [773], Average Loss: 0.6803
Training -- Epoch [1], Sample [774], Average Loss: 0.6795
Training -- Epoch [1], Sample [775], Average Loss: 0.6800
Training -- Epoch [1], Sample [776], Average Loss: 0.6796
Training -- Epoch [1], Sample [777], Average Loss: 0.6801
Training -- Epoch [1], Sample [778], Average Loss: 0.6795
Training -- Epoch [1], Sample [779], Average Loss: 0.6787
Training -- Epoch [1], Sample [780], Average Loss: 0.6786
Training -- Epoch [1], Sample [781], Average Loss: 0.6779
Training -- Epoch [1], Sample [782], Average Loss: 0.6776
Training -- Epoch [1], Sample [783], Average Loss: 0.6769
Training -- Epoch [1], Sample [784], Average Loss: 0.6763
Training -- Epoch [1], Sample [785], Average Loss: 0.6763
Training -- Epoch [1], Sample [786], Average Loss: 0.6787
Training -- Epoch [1], Sample [787], Average Loss: 0.6783
Training -- Epoch [1], Sample [788], Average Loss: 0.6795
Training -- Epoch [1], Sample [789], Average Loss: 0.6796
Training -- Epoch [1], Sample [790], Average Loss: 0.6801
Training -- Epoch [1], Sample [791], Average Loss: 0.6794
Training -- Epoch [1], Sample [792], Average Loss: 0.6787
Training -- Epoch [1], Sample [793], Average Loss: 0.6782
Training -- Epoch [1], Sample [794], Average Loss: 0.6774
Training -- Epoch [1], Sample [795], Average Loss: 0.6778
Training -- Epoch [1], Sample [796], Average Loss: 0.6792
Training -- Epoch [1], Sample [797], Average Loss: 0.6792
Training -- Epoch [1], Sample [798], Average Loss: 0.6785
Training -- Epoch [1], Sample [799], Average Loss: 0.6781
Training -- Epoch [1], Sample [800], Average Loss: 0.6773
Training -- Epoch [1], Sample [801], Average Loss: 0.6772
Training -- Epoch [1], Sample [802], Average Loss: 0.6766
Training -- Epoch [1], Sample [803], Average Loss: 0.6782
Training -- Epoch [1], Sample [804], Average Loss: 0.6783
Training -- Epoch [1], Sample [805], Average Loss: 0.6775
Training -- Epoch [1], Sample [806], Average Loss: 0.6772
Training -- Epoch [1], Sample [807], Average Loss: 0.6771
Training -- Epoch [1], Sample [808], Average Loss: 0.6770
Training -- Epoch [1], Sample [809], Average Loss: 0.6765
Training -- Epoch [1], Sample [810], Average Loss: 0.6762
Training -- Epoch [1], Sample [811], Average Loss: 0.6755
Training -- Epoch [1], Sample [812], Average Loss: 0.6748
Training -- Epoch [1], Sample [813], Average Loss: 0.6748
Training -- Epoch [1], Sample [814], Average Loss: 0.6779
Training -- Epoch [1], Sample [815], Average Loss: 0.6771
Training -- Epoch [1], Sample [816], Average Loss: 0.6774
Training -- Epoch [1], Sample [817], Average Loss: 0.6765
Training -- Epoch [1], Sample [818], Average Loss: 0.6757
Training -- Epoch [1], Sample [819], Average Loss: 0.6750
Training -- Epoch [1], Sample [820], Average Loss: 0.6745
Training -- Epoch [1], Sample [821], Average Loss: 0.6737
Training -- Epoch [1], Sample [822], Average Loss: 0.6735
Training -- Epoch [1], Sample [823], Average Loss: 0.6727
Training -- Epoch [1], Sample [824], Average Loss: 0.6721
Training -- Epoch [1], Sample [825], Average Loss: 0.6742
Training -- Epoch [1], Sample [826], Average Loss: 0.6737
Training -- Epoch [1], Sample [827], Average Loss: 0.6730
Training -- Epoch [1], Sample [828], Average Loss: 0.6726
Training -- Epoch [1], Sample [829], Average Loss: 0.6719
Training -- Epoch [1], Sample [830], Average Loss: 0.6711
Training -- Epoch [1], Sample [831], Average Loss: 0.6708
Training -- Epoch [1], Sample [832], Average Loss: 0.6701
Training -- Epoch [1], Sample [833], Average Loss: 0.6697
Training -- Epoch [1], Sample [834], Average Loss: 0.6690
Training -- Epoch [1], Sample [835], Average Loss: 0.6685
Training -- Epoch [1], Sample [836], Average Loss: 0.6698
Training -- Epoch [1], Sample [837], Average Loss: 0.6700
Training -- Epoch [1], Sample [838], Average Loss: 0.6693
Training -- Epoch [1], Sample [839], Average Loss: 0.6687
Training -- Epoch [1], Sample [840], Average Loss: 0.6679
Training -- Epoch [1], Sample [841], Average Loss: 0.6688
Training -- Epoch [1], Sample [842], Average Loss: 0.6680
Training -- Epoch [1], Sample [843], Average Loss: 0.6674
Training -- Epoch [1], Sample [844], Average Loss: 0.6671
Training -- Epoch [1], Sample [845], Average Loss: 0.6664
Training -- Epoch [1], Sample [846], Average Loss: 0.6658
Training -- Epoch [1], Sample [847], Average Loss: 0.6651
Training -- Epoch [1], Sample [848], Average Loss: 0.6653
Training -- Epoch [1], Sample [849], Average Loss: 0.6646
Training -- Epoch [1], Sample [850], Average Loss: 0.6643
Training -- Epoch [1], Sample [851], Average Loss: 0.6645
Training -- Epoch [1], Sample [852], Average Loss: 0.6637
Training -- Epoch [1], Sample [853], Average Loss: 0.6649
Training -- Epoch [1], Sample [854], Average Loss: 0.6642
Training -- Epoch [1], Sample [855], Average Loss: 0.6642
Training -- Epoch [1], Sample [856], Average Loss: 0.6635
Training -- Epoch [1], Sample [857], Average Loss: 0.6628
Training -- Epoch [1], Sample [858], Average Loss: 0.6623
Training -- Epoch [1], Sample [859], Average Loss: 0.6619
Training -- Epoch [1], Sample [860], Average Loss: 0.6613
Training -- Epoch [1], Sample [861], Average Loss: 0.6609
Training -- Epoch [1], Sample [862], Average Loss: 0.6602
Training -- Epoch [1], Sample [863], Average Loss: 0.6596
Training -- Epoch [1], Sample [864], Average Loss: 0.6591
Training -- Epoch [1], Sample [865], Average Loss: 0.6585
Training -- Epoch [1], Sample [866], Average Loss: 0.6579
Training -- Epoch [1], Sample [867], Average Loss: 0.6573
Training -- Epoch [1], Sample [868], Average Loss: 0.6573
Training -- Epoch [1], Sample [869], Average Loss: 0.6572
Training -- Epoch [1], Sample [870], Average Loss: 0.6565
Training -- Epoch [1], Sample [871], Average Loss: 0.6568
Training -- Epoch [1], Sample [872], Average Loss: 0.6562
Training -- Epoch [1], Sample [873], Average Loss: 0.6555
Training -- Epoch [1], Sample [874], Average Loss: 0.6550
Training -- Epoch [1], Sample [875], Average Loss: 0.6544
Training -- Epoch [1], Sample [876], Average Loss: 0.6538
Training -- Epoch [1], Sample [877], Average Loss: 0.6538
Training -- Epoch [1], Sample [878], Average Loss: 0.6533
Training -- Epoch [1], Sample [879], Average Loss: 0.6547
Training -- Epoch [1], Sample [880], Average Loss: 0.6540
Training -- Epoch [1], Sample [881], Average Loss: 0.6534
Training -- Epoch [1], Sample [882], Average Loss: 0.6526
Training -- Epoch [1], Sample [883], Average Loss: 0.6523
Training -- Epoch [1], Sample [884], Average Loss: 0.6524
Training -- Epoch [1], Sample [885], Average Loss: 0.6517
Training -- Epoch [1], Sample [886], Average Loss: 0.6512
Training -- Epoch [1], Sample [887], Average Loss: 0.6505
Training -- Epoch [1], Sample [888], Average Loss: 0.6499
Training -- Epoch [1], Sample [889], Average Loss: 0.6495
Training -- Epoch [1], Sample [890], Average Loss: 0.6492
Training -- Epoch [1], Sample [891], Average Loss: 0.6499
Training -- Epoch [1], Sample [892], Average Loss: 0.6492
Training -- Epoch [1], Sample [893], Average Loss: 0.6485
Training -- Epoch [1], Sample [894], Average Loss: 0.6479
Training -- Epoch [1], Sample [895], Average Loss: 0.6472
Training -- Epoch [1], Sample [896], Average Loss: 0.6482
Training -- Epoch [1], Sample [897], Average Loss: 0.6477
Training -- Epoch [1], Sample [898], Average Loss: 0.6483
Training -- Epoch [1], Sample [899], Average Loss: 0.6486
Training -- Epoch [1], Sample [900], Average Loss: 0.6480
Training -- Epoch [1], Sample [901], Average Loss: 0.6484
Training -- Epoch [1], Sample [902], Average Loss: 0.6489
Training -- Epoch [1], Sample [903], Average Loss: 0.6493
Training -- Epoch [1], Sample [904], Average Loss: 0.6493
Training -- Epoch [1], Sample [905], Average Loss: 0.6487
Training -- Epoch [1], Sample [906], Average Loss: 0.6485
Training -- Epoch [1], Sample [907], Average Loss: 0.6488
Training -- Epoch [1], Sample [908], Average Loss: 0.6490
Training -- Epoch [1], Sample [909], Average Loss: 0.6483
Training -- Epoch [1], Sample [910], Average Loss: 0.6497
Training -- Epoch [1], Sample [911], Average Loss: 0.6492
Training -- Epoch [1], Sample [912], Average Loss: 0.6485
Training -- Epoch [1], Sample [913], Average Loss: 0.6478
Training -- Epoch [1], Sample [914], Average Loss: 0.6471
Training -- Epoch [1], Sample [915], Average Loss: 0.6465
Training -- Epoch [1], Sample [916], Average Loss: 0.6461
Training -- Epoch [1], Sample [917], Average Loss: 0.6455
Training -- Epoch [1], Sample [918], Average Loss: 0.6451
Training -- Epoch [1], Sample [919], Average Loss: 0.6447
Training -- Epoch [1], Sample [920], Average Loss: 0.6445
Training -- Epoch [1], Sample [921], Average Loss: 0.6442
Training -- Epoch [1], Sample [922], Average Loss: 0.6436
Training -- Epoch [1], Sample [923], Average Loss: 0.6430
Training -- Epoch [1], Sample [924], Average Loss: 0.6425
Training -- Epoch [1], Sample [925], Average Loss: 0.6422
Training -- Epoch [1], Sample [926], Average Loss: 0.6416
Training -- Epoch [1], Sample [927], Average Loss: 0.6409
Training -- Epoch [1], Sample [928], Average Loss: 0.6421
Training -- Epoch [1], Sample [929], Average Loss: 0.6426
Training -- Epoch [1], Sample [930], Average Loss: 0.6425
Training -- Epoch [1], Sample [931], Average Loss: 0.6455
Training -- Epoch [1], Sample [932], Average Loss: 0.6450
Training -- Epoch [1], Sample [933], Average Loss: 0.6453
Training -- Epoch [1], Sample [934], Average Loss: 0.6448
Training -- Epoch [1], Sample [935], Average Loss: 0.6445
Training -- Epoch [1], Sample [936], Average Loss: 0.6445
Training -- Epoch [1], Sample [937], Average Loss: 0.6441
Training -- Epoch [1], Sample [938], Average Loss: 0.6435
Training -- Epoch [1], Sample [939], Average Loss: 0.6430
Training -- Epoch [1], Sample [940], Average Loss: 0.6426
Training -- Epoch [1], Sample [941], Average Loss: 0.6419
Training -- Epoch [1], Sample [942], Average Loss: 0.6422
Training -- Epoch [1], Sample [943], Average Loss: 0.6421
Training -- Epoch [1], Sample [944], Average Loss: 0.6424
Training -- Epoch [1], Sample [945], Average Loss: 0.6418
Training -- Epoch [1], Sample [946], Average Loss: 0.6413
Training -- Epoch [1], Sample [947], Average Loss: 0.6419
Training -- Epoch [1], Sample [948], Average Loss: 0.6422
Training -- Epoch [1], Sample [949], Average Loss: 0.6417
Training -- Epoch [1], Sample [950], Average Loss: 0.6422
Training -- Epoch [1], Sample [951], Average Loss: 0.6420
Training -- Epoch [1], Sample [952], Average Loss: 0.6415
Training -- Epoch [1], Sample [953], Average Loss: 0.6410
Training -- Epoch [1], Sample [954], Average Loss: 0.6403
Training -- Epoch [1], Sample [955], Average Loss: 0.6405
Training -- Epoch [1], Sample [956], Average Loss: 0.6401
Training -- Epoch [1], Sample [957], Average Loss: 0.6395
Training -- Epoch [1], Sample [958], Average Loss: 0.6414
Training -- Epoch [1], Sample [959], Average Loss: 0.6413
Training -- Epoch [1], Sample [960], Average Loss: 0.6408
Training -- Epoch [1], Sample [961], Average Loss: 0.6405
Training -- Epoch [1], Sample [962], Average Loss: 0.6400
Training -- Epoch [1], Sample [963], Average Loss: 0.6395
Training -- Epoch [1], Sample [964], Average Loss: 0.6391
Training -- Epoch [1], Sample [965], Average Loss: 0.6385
Training -- Epoch [1], Sample [966], Average Loss: 0.6378
Training -- Epoch [1], Sample [967], Average Loss: 0.6391
Training -- Epoch [1], Sample [968], Average Loss: 0.6389
Training -- Epoch [1], Sample [969], Average Loss: 0.6385
Training -- Epoch [1], Sample [970], Average Loss: 0.6380
Training -- Epoch [1], Sample [971], Average Loss: 0.6380
Training -- Epoch [1], Sample [972], Average Loss: 0.6374
Training -- Epoch [1], Sample [973], Average Loss: 0.6371
Training -- Epoch [1], Sample [974], Average Loss: 0.6365
Training -- Epoch [1], Sample [975], Average Loss: 0.6362
Training -- Epoch [1], Sample [976], Average Loss: 0.6357
Training -- Epoch [1], Sample [977], Average Loss: 0.6350
Training -- Epoch [1], Sample [978], Average Loss: 0.6345
Training -- Epoch [1], Sample [979], Average Loss: 0.6339
Training -- Epoch [1], Sample [980], Average Loss: 0.6334
Training -- Epoch [1], Sample [981], Average Loss: 0.6329
Training -- Epoch [1], Sample [982], Average Loss: 0.6326
Training -- Epoch [1], Sample [983], Average Loss: 0.6339
Training -- Epoch [1], Sample [984], Average Loss: 0.6341
Training -- Epoch [1], Sample [985], Average Loss: 0.6336
Training -- Epoch [1], Sample [986], Average Loss: 0.6331
Training -- Epoch [1], Sample [987], Average Loss: 0.6328
Training -- Epoch [1], Sample [988], Average Loss: 0.6322
Training -- Epoch [1], Sample [989], Average Loss: 0.6320
Training -- Epoch [1], Sample [990], Average Loss: 0.6319
Training -- Epoch [1], Sample [991], Average Loss: 0.6314
Training -- Epoch [1], Sample [992], Average Loss: 0.6319
Training -- Epoch [1], Sample [993], Average Loss: 0.6333
Training -- Epoch [1], Sample [994], Average Loss: 0.6327
Training -- Epoch [1], Sample [995], Average Loss: 0.6335
Training -- Epoch [1], Sample [996], Average Loss: 0.6330